The activation function activates the neural network through its nonlinear mechanism, allowing it to work effectively and maintain high performance. Therefore, choosing a good activation function can have a significant impact on the training results of the network model. Due to its simplicity and high efficiency, ReLU was once used as the standard activation function for various applications in the field of deep learning. Despite this, related research has not been interrupted. In recent years, the gated activation function Swish and Mish have been proposed successively, all of which perform well. Inspired by this, we consider designing a similar function consisting of learnable parameters and hyperparameters in order to train higher performance neural network models. In the process of function design, we found that if no restrictions were applied, the value range of learnable parameters would become too large and out of control, which would cause abnormal fluctuations of the gradient in the training process, making it difficult to further improve the performance of the network model. In order to solve this problem, we propose a new activation function Pish, which can generate a gradient space conducive to neural network training by limiting the values of learnable parameters and hyperparameters to a specific range, thus further improving the accuracy of the model. Four publicly available datasets CIFAR-10, CIFAR-100, STL10, and SVHN were used for the experiments. ResNet-50, DenseNet-121, MobileNet, SqueezeNet, and Se ResNet-18 were selected as models. The final results showed that the top-1 test accuracy of Pish was better than that of ReLU, Swish, and Mish.
